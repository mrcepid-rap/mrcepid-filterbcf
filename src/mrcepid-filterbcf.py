#!/usr/bin/env python
# mrcepid-filterbcf 1.0.0
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/
import csv
import dxpy

from pathlib import Path
from time import sleep
from typing import TypedDict, List

from general_utilities.association_resources import download_dxfile_by_name
from general_utilities.job_management.command_executor import CommandExecutor
from general_utilities.job_management.thread_utility import ThreadUtility

from filterbcf.ingest_data import IngestData, AdditionalAnnotation
from filterbcf.vcf_filter.vcf_filter import VCFFilter
from filterbcf.vcf_annotate.vcf_annotate import VCFAnnotate


class ProcessedReturn(TypedDict):
    chrom: str
    start: int
    end: int
    vcf_prefix: str
    output_bcf: dxpy.DXFile
    output_bcf_idx: dxpy.DXFile
    output_vep: dxpy.DXFile
    output_vep_idx: dxpy.DXFile


# This is a method that will execute all steps necessary to process one VCF file
# It is the primary unit that is executed by individual threads from the 'main()' method
def process_vcf(vcf: str, additional_annotations: List[AdditionalAnnotation],
                cmd_executor: CommandExecutor, cadd_executor: CommandExecutor) -> ProcessedReturn:

    # Download the VCF file chunk to the instance
    vcf_path = download_dxfile_by_name(vcf, project_id=dxpy.PROJECT_CONTEXT_ID, print_status=False)

    # 1. Do normalisation and filtering
    vcf_filter = VCFFilter(vcf_path, cmd_executor)

    # We need to pause here in each thread to make sure that CADD and VEP files have downloaded in separate threads...
    # We know that when the original .tar.gz files are gone it is safe proceed; deleting these files is the final step
    # of the download process.
    downloads = [Path('reference.fasta.gz'),
                 Path('loftee_hg38.tar.gz'),
                 Path('homo_sapiens_vep_108_GRCh38.tar.gz'),
                 Path('annotationsGRCh38_v1.6.tar.gz')]
    cadd_index = Path('cadd_precomputed/gnomad.genomes.r3.0.indel.tsv.gz.tbi')

    while True in [file.exists() for file in downloads] and cadd_index.exists() is False:
        sleep(5)

    # 2. Do annotation
    vcf_annotater = VCFAnnotate(vcf_path, vcf_filter.filtered_vcf, additional_annotations, cmd_executor, cadd_executor)

    return {'chrom': vcf_annotater.chunk_chrom,
            'start': vcf_annotater.chunk_start,
            'end': vcf_annotater.chunk_end,
            'vcf_prefix': vcf_path.stem,
            'output_bcf': vcf_annotater.finalbcf,
            'output_bcf_idx': vcf_annotater.finalbcf_index,
            'output_vep': vcf_annotater.finalvep,
            'output_vep_idx': vcf_annotater.finalvep_index}


@dxpy.entry_point('main')
def main(input_vcfs: dict, coordinates_name: str, human_reference: dict, human_reference_index: dict,
         vep_cache: dict, loftee_libraries: dict, cadd_annotations: dict, precomputed_cadd_snvs: dict,
         precomputed_cadd_indels: dict, additional_annotations: List[dict]):

    # Build a thread worker that contains as many threads, divided by 2 that have been requested since each bcftools
    # 1 thread for monitoring threads
    # 2 threads for downloading (1 each for CADD and VEP)
    # 2 threads for each BCF
    thread_utility = ThreadUtility(thread_factor=4, error_message='A bcffiltering thread failed', incrementor=5)

    # Separate function to acquire necessary resource files
    # We pass the above thread utility here to ensure that the download threads are managed by the same thread utility
    ingested_data = IngestData(input_vcfs, human_reference, human_reference_index, vep_cache, loftee_libraries,
                               cadd_annotations, precomputed_cadd_snvs, precomputed_cadd_indels, additional_annotations,
                               thread_utility)

    # And launch the requested threads
    for input_vcf in ingested_data.input_vcfs:
        thread_utility.launch_job(process_vcf,
                                  vcf=input_vcf,
                                  additional_annotations=ingested_data.annotations,
                                  cmd_executor=ingested_data.cmd_executor,
                                  cadd_executor=ingested_data.cadd_executor)

    print("All threads submitted...")

    # And add the resulting futures to relevant output arrays / file
    output_bcfs = []
    output_bcf_idxs = []
    output_veps = []
    output_vep_idxs = []

    # This file contains information about the 'chunks' that have been processed. It DOES NOT have a header to
    # make it easier to concatenate coordinate files from multiple runs. The columns are as follows:
    # [0] #chrom
    # [1] start
    # [2] end
    # [3] chunk_prefix
    # [4] bcf_dxpy
    # [5] vep_dxpy

    with Path(coordinates_name).open('w') as coordinate_writer:

        coordinate_csv = csv.DictWriter(coordinate_writer,
                                        fieldnames=['chrom', 'start', 'end', 'vcf_prefix',
                                                    'output_bcf', 'output_bcf_idx', 'output_vep', 'output_vep_idx'],
                                        delimiter="\t")

        coordinate_csv.writeheader()

        # And gather the resulting futures
        for result in thread_utility:
            # This 'if' statement has to be here because of download threads I run simultaneously.
            # For some reason, when a 'None' result is returned, it doesn't trigger an exception and the entire process
            # just hangs for eternity.
            if result is None:
                continue
            output_bcfs.append(result['output_bcf'])
            output_bcf_idxs.append(result['output_bcf_idx'])
            output_veps.append(result['output_vep'])
            output_vep_idxs.append(result['output_vep_idx'])
            writer_dict = {
                'chrom': result['chrom'],
                'start': result['start'],
                'end': result['end'],
                'vcf_prefix': result['vcf_prefix'],
                'output_bcf': result['output_bcf'].describe()['id'],
                'output_bcf_idx': result['output_bcf_idx'].describe()['id'],
                'output_vep': result['output_vep'].describe()['id'],
                'output_vep_idx': result['output_vep_idx'].describe()['id']}
            coordinate_csv.writerow(writer_dict)

    print("All threads completed...")

    # Getting files back into your project directory on DNAnexus is a two-step process:
    # 1. uploading the local file to the DNA nexus platform to assign it a file-ID (looks like file-ABCDEFGHIJKLMN1234567890)
    # 2. linking this file ID to your project and placing it within your project's directory structure
    # (the subdirectory can be controlled on the command-line by adding a flag to `dx run` like: --destination test/)
    output = {"output_bcfs": [dxpy.dxlink(item) for item in output_bcfs],
              "output_bcf_idxs": [dxpy.dxlink(item) for item in output_bcf_idxs],
              "output_veps": [dxpy.dxlink(item) for item in output_veps],
              "output_vep_idxs": [dxpy.dxlink(item) for item in output_vep_idxs],
              "coordinates_file": dxpy.dxlink(dxpy.upload_local_file(coordinates_name))}

    # This returns all the information about your exit files to the work managing your job via DNANexus:
    return output


dxpy.run()
