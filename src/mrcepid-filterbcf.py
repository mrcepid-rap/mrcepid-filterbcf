#!/usr/bin/env python
# mrcepid-filterbcf 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import dxpy
import subprocess
import csv


# This function runs a command on an instance, either with or without calling the docker instance we downloaded
# By default, commands are not run via Docker, but can be changed by setting is_docker = True
def run_cmd(cmd, is_docker=False):

    if is_docker:
        # -v here mounts a local directory on an instance (in this case the home dir) to a directory internal to the
        # Docker instance named /test/. This allows us to run commands on files stored on the AWS instance within Docker
        cmd = "docker run -v /home/dnanexus:/test egardner413/mrcepid-filtering " + cmd

    # Standard python calling external commands protocol
    print(cmd)
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()

    # If the command doesn't work, print the error stream and close the AWS instance out with 'dxpy.AppError'
    if proc.returncode != 0:
        print("The following cmd failed:")
        print(cmd)
        print("STDERROR follows\n")
        print(stderr.decode('utf-8'))
        raise dxpy.AppError("Failed to run properly...")


# Utility function to delete files no longer needed from the AWS instance to save space
def purge_file(file):

    cmd = "rm " + file
    run_cmd(cmd)


# This is just to compartmentalise the collection of all the resources I need for this task and
# get them into the right place
def ingest_resources():

    # Here we are downloading & unpacking resource files that are required for the annotation engine, they are:
    # 1. Human reference files
    reference_file = dxpy.DXFile(
        'file-Fx2x270Jx0j17zkb3kbBf6q2')  # This is the location of the GRCh38 reference file on AWS London
    reference_index = dxpy.DXFile(
        'file-Fx2x21QJ06f47gV73kZPjkQQ')  # This is the location of the GRCh38 reference index file on AWS London
    dxpy.download_dxfile(reference_file.get_id(), "reference.fasta.gz") # This actually downloads the file onto the current instance
    dxpy.download_dxfile(reference_index.get_id(), "reference.fasta.fai")
    cmd = "gunzip reference.fasta.gz" # Better to unzip the reference for most commands...
    run_cmd(cmd)

    # 2. vep cache file
    dxpy.download_folder('project-G2XK5zjJXk83yZ598Z7BpGPk',
                         'vep_caches/',
                         folder = "/project_resources/vep_caches/")
    cmd = "tar -zxf vep_caches/homo_sapiens_vep_104_GRCh38.tar.gz -C vep_caches/"
    run_cmd(cmd)

    ## 3. loftee reference files:
    dxpy.download_folder('project-G2XK5zjJXk83yZ598Z7BpGPk',
                         'loftee_files/',
                         folder = "/project_resources/loftee_files/")
    cmd = "tar -zxf loftee_files/loftee_hg38.tar.gz -C loftee_files/"
    run_cmd(cmd)

    ## 4. gnomAD MAF files:
    dxpy.download_folder('project-G2XK5zjJXk83yZ598Z7BpGPk',
                         'gnomad_files/',
                         folder = "/project_resources/gnomad_files/")

    ## 5. REVEL files
    dxpy.download_folder('project-G2XK5zjJXk83yZ598Z7BpGPk',
                         'revel_files/',
                         folder = "/project_resources/revel_files/")


def do_filtering(vcfprefix):

    # Do genotype level filtering
    # -S : sets genotypes which fail -i to missing (./.)
    # -i : filtering expression to decide whether to set to missing or not.
        # Note this is to INCLUDE pass genotypes, not EXCLUDE fail genotypes
        # Filtering is split up based on variant type (snp / indel) and genotype due to issues in variant
        # calling in the UK Biobank WES VCFs
        # For snps:
        # - all genotypes are filtered on depth (DP) ≥ 7
        # - homozygous REF ("RR") are only filtered on genotype quality (GQ) ≥ 20
        # - heterozygous ("RA") are filtered on GQ ≥ 20 and a ref/alt balance binomial test [binom()] p.value of 1e-3
        # - homozygous ALT ("AA") are only filtered on DP due to issues with GQ for hom alt alleles being improperly assigned
        # For InDels:
        # - All variants regardless of genotype are filtered on DP ≥ 10 and GQ ≥ 20
    cmd = "bcftools filter -Oz -o /test/variants.norm.filtered.vcf.gz --threads 4 -S . " \
          "-i '(TYPE=\"snp\" & FMT/DP >= 7 & ((FMT/GT=\"RR\" & FMT/GQ >= 20) | " \
          "(FMT/GT=\"RA\" & FMT/GQ >= 20 & binom(FMT/AD) > 0.001) | " \
          "(FMT/GT=\"AA\"))) | " \
          "(TYPE=\"indel\" & FMT/DP >= 10 & FMT/GQ >= 20)' " \
          "/test/variants.norm.vcf.gz"
    run_cmd(cmd, True)

    # Add values for missingness and AC/AF
    # +fill-tags is a bcftools nonstandard plugin which calculates INFO fields from available genotypes. Note the non-standard '--' at then end which provides
    # commands to the plugin rather than bcftools itself
    # INFO fields added are:
    # F_MISSING: fraction of missing genotypes
    # AC       : allele count of ALT allele
    # AF       : allele frequency of ALT allele
    # AN       : number of possible alleles (accounting for missingness)
    cmd = "bcftools +fill-tags /test/variants.norm.filtered.vcf.gz -Oz -o /test/variants.norm.filtered.tagged.vcf.gz -- -t F_MISSING,AC,AF,AN"
    run_cmd(cmd, True)
    purge_file("variants.norm.filtered.vcf.gz")

    # Generate a file for pre-post filtering comparison:
    # This is the same as above, but just to make sure we have statistics from the unfiltered file to compare to
    cmd = "bcftools +fill-tags /test/variants.norm.vcf.gz -Oz -o /test/variants.norm.tagged.vcf.gz -- -t F_MISSING,AC,AF,AN"
    run_cmd(cmd, True)
    purge_file("variants.norm.vcf.gz")

    # Set pass/fail filters within the filtered VCF
    # -s : sets SITES that fail the filtering expression from -i are set to FAIL
    # -i : only include sites as PASS if they meet these requirements
        # F_MISSING : Only include sits with less than 50% missing genotypes
        # AC        : Only include biallelic sites (i.e. no monomorphic)
    cmd = "bcftools filter -i \'F_MISSING<=0.50 & AC!=0\' -s \'FAIL\' -Oz -o /test/variants.norm.filtered.tagged.missingness_filtered.vcf.gz --threads 4 " \
          "/test/variants.norm.filtered.tagged.vcf.gz"
    run_cmd(cmd, True)
    purge_file("variants.norm.filtered.tagged.vcf.gz")

    ## And generate some stats:
    # Per Site
    # From the filtered VCF
    cmd = "bcftools query -f \"%CHROM\\t%POS\\t%FILTER\\t%F_MISSING\\t%AC\\t%AN\\n\" -o /test/" + vcfprefix + ".filtered_site_stats.tsv /test/variants.norm.filtered.tagged.missingness_filtered.vcf.gz"
    run_cmd(cmd, True)
    cmd = "gzip " + vcfprefix + ".filtered_site_stats.tsv"
    run_cmd(cmd)
    # From the unfiltered VCF
    cmd = "bcftools query -f \"%CHROM\\t%POS\\t%FILTER\\t%F_MISSING\\t%AC\\t%AN\\n\" -o /test/" + vcfprefix + ".unfiltered_site_stats.tsv /test/variants.norm.tagged.vcf.gz"
    run_cmd(cmd, True)
    cmd = "gzip " + vcfprefix + ".unfiltered_site_stats.tsv"
    run_cmd(cmd)

    # Per Sample
    # Get the list of samples queried
    cmd = "bcftools query -l /test/variants.norm.filtered.tagged.missingness_filtered.vcf.gz > UKBB_samples.txt"
    run_cmd(cmd, True)
    # From the filtered VCF
    cmd = "bcftools stats --threads 4 -S /test/UKBB_samples.txt /test/variants.norm.filtered.tagged.missingness_filtered.vcf.gz > " + vcfprefix + ".filtered_sample_stats.tsv"
    run_cmd(cmd, True)
    cmd = "gzip " + vcfprefix + ".filtered_sample_stats.tsv"
    run_cmd(cmd)
    # From the unfiltered VCF
    cmd = "bcftools stats --threads 4 -S /test/UKBB_samples.txt /test/variants.norm.tagged.vcf.gz > " + vcfprefix + ".unfiltered_sample_stats.tsv"
    run_cmd(cmd, True)
    cmd = "gzip " + vcfprefix + ".unfiltered_sample_stats.tsv"
    run_cmd(cmd)
    purge_file("variants.norm.tagged.vcf.gz")


def run_vep():

    # Generate a file without genotypes for VEP
    # -G : strips genotypes
    cmd = "bcftools view --threads 4 -G -Oz -o /test/variants.norm.filtered.tagged.missingness_filtered.sites.vcf.gz /test/variants.norm.filtered.tagged.missingness_filtered.vcf.gz"
    run_cmd(cmd, True)

    # Then run VEP on the resulting file:
    # Not going to document each individual thing for VEP, but all are available on the VEP webiste
    cmd = "perl -Iensembl-vep/cache/Plugins/loftee/ -Iensembl-vep/cache/Plugins/loftee/maxEntScan/ " \
          "ensembl-vep/vep --offline --cache --assembly GRCh38 --dir_cache /test/vep_caches/ --everything --allele_num " \
          "-i /test/variants.norm.filtered.tagged.missingness_filtered.sites.vcf.gz --format vcf --fasta /test/reference.fasta " \
          "-o /test/variants.norm.filtered.tagged.missingness_filtered.sites.vep.vcf.gz --compress_output bgzip --vcf " \
          "--dir_plugins ensembl-vep/cache/Plugins/ " \
          "--plugin LoF,loftee_path:ensembl-vep/cache/Plugins/loftee,human_ancestor_fa:/test/loftee_files/loftee_hg38/human_ancestor.fa.gz,conservation_file:/test/loftee_files/loftee_hg38/loftee.sql,gerp_bigwig:/test/loftee_files/loftee_hg38/gerp_conservation_scores.homo_sapiens.GRCh38.bw " \
          "--plugin REVEL,/test/revel_files/new_tabbed_revel_grch38.tsv.gz"
    run_cmd(cmd, True)
    purge_file("variants.norm.filtered.tagged.missingness_filtered.sites.vcf.gz")

    # Add better gnomAD MAF information
    # bcftools just takes a tabix-formated tsv file (in this case gnomad.tsv.gz) and adds non-coordinate fields as INFO fields
    # In this case, the gnomad.tsv.gz file has 6 columns, and I ignore column 5
    # Thus, an INFO field named "gnomAD_MAF" is added to the VEP-annotated VCF
    # There is also a file (gnomad.header.txt) that contains the INFO field annotation for the vcf header, it is just one line:
        # ##INFO=<ID=gnomAD_MAF,Number=1,Type=Float,Description="gnomAD Exomes AF">
    cmd = "bcftools annotate --threads 4 -a /test/gnomad_files/gnomad.tsv.gz -c CHROM,POS,REF,ALT,-,gnomAD_MAF -h /test/gnomad_files/gnomad.header.txt -Oz -o /test/variants.norm.filtered.tagged.missingness_filtered.sites.vep.gnomad.vcf.gz /test/variants.norm.filtered.tagged.missingness_filtered.sites.vep.vcf.gz"
    run_cmd(cmd, True)
    purge_file("variants.norm.filtered.tagged.missingness_filtered.sites.vep.vcf.gz")

    # Then generate a output TSV that we can parse:
    # The purpose of this is to generate a TSV file of annotations that we care about to parse later (function parse_vep())
    # +split-vep is a bcftools plugin that iterates through VEP fields provided in a VCF via the CSQ INFO field.
        # -d :  outputs duplicate transcripts on separate lines. In other words, a gene may have multiple transcripts,
        # and put each transcript and CSQ on a separate line in the tsv file
        # -f : CSQ fields that we want to output into the tsv file
    cmd = "bcftools +split-vep -df '%CHROM\\t%POS\\t%REF\\t%ALT\\t%ID\\t%FILTER\\t%INFO/AF\\t%F_MISSING\\t%AN\\t%AC\\t%MANE_SELECT\\t%Feature\\t%Gene\\t%BIOTYPE\\t%CANONICAL\\t%SYMBOL\\t%Consequence\\t%gnomAD_MAF\\t%REVEL\\t%SIFT\\t%PolyPhen\\t%LoF\\n' -o /test/variants.vep_table.tsv /test/variants.norm.filtered.tagged.missingness_filtered.sites.vep.gnomad.vcf.gz"
    run_cmd(cmd, True)
    purge_file("variants.norm.filtered.tagged.missingness_filtered.sites.vep.gnomad.vcf.gz")


# Writes a VCF style header that is compatible with bcftools annotate for adding VEP info back into our filtered VCF
def write_annote_header():

    header_writer = open('variants.header.txt', 'w')
    header_writer.writelines('##INFO=<ID=MANE,Number=1,Type=String,Description="Canonical MANE Transcript">' + "\n")
    header_writer.writelines('##INFO=<ID=ENST,Number=1,Type=String,Description="Canonical Ensembl Transcript">' + "\n")
    header_writer.writelines('##INFO=<ID=ENSG,Number=1,Type=String,Description="Canonical Ensembl Gene">' + "\n")
    header_writer.writelines('##INFO=<ID=BIOTYPE,Number=1,Type=String,Description="Biotype of ENSG as in VEP">' + "\n")
    header_writer.writelines('##INFO=<ID=SYMBOL,Number=1,Type=String,Description="HGNC Gene ID">' + "\n")
    header_writer.writelines('##INFO=<ID=CSQ,Number=1,Type=String,Description="Most severe VEP CSQ for this variant">' + "\n")
    header_writer.writelines('##INFO=<ID=gnomAD_AF,Number=1,Type=Float,Description="gnomAD v3.0 Exomes AF. If 0,'
                             ' variant does not exist in gnomAD">' + "\n")
    header_writer.writelines('##INFO=<ID=REVEL,Number=1,Type=Float,Description="REVEL Score. '
                             'NaN if not a missense variant.">' + "\n")
    header_writer.writelines('##INFO=<ID=SIFT,Number=1,Type=String,Description="SIFT Score. '
                             'NA if not a missense variant.">' + "\n")
    header_writer.writelines('##INFO=<ID=POLYPHEN,Number=1,Type=String,Description="PolyPhen Score. '
                             'NA if not a missense variant.">' + "\n")
    header_writer.writelines('##INFO=<ID=LOFTEE,Number=1,Type=String,Description="LOFTEE annotation if LoF CSQ. '
                             'NA if not a PTV.">' + "\n")
    header_writer.writelines('##INFO=<ID=PARSED_CSQ,Number=1,Type=String,Description="Parsed simplified CSQ">' + "\n")
    header_writer.writelines('##INFO=<ID=MULTI,Number=1,Type=String,Description="Is variant multiallelic?">' + "\n")
    header_writer.writelines('##INFO=<ID=INDEL,Number=1,Type=String,Description="Is variant an InDel?">' + "\n")
    header_writer.writelines('##INFO=<ID=MINOR,Number=1,Type=String,Description="Minor Allele">' + "\n")
    header_writer.writelines('##INFO=<ID=MAJOR,Number=1,Type=String,Description="Major Allele">' + "\n")
    header_writer.writelines('##INFO=<ID=MAF,Number=1,Type=Float,Description="Minor Allele Frequency">' + "\n")
    header_writer.writelines('##INFO=<ID=MAC,Number=1,Type=Float,Description="Minor Allele Count">' + "\n")
    header_writer.close()


# Decide how "severe" a CSQ is for a given annotation record
def define_score(csqs):

    csqs_split = csqs.split('&')

    # This is a Eugene-decided level of severity partially decided based on VEP severity score.
    # This should contain all possible CSQ annotations for a variant other than those reserved for large structural variants
    vep_consequences = {'stop_gained': {'score': 1, 'type': 'PTV'},
                        'frameshift_variant': {'score': 2, 'type': 'PTV'},
                        'splice_acceptor_variant': {'score': 3, 'type': 'PTV'},
                        'splice_donor_variant': {'score': 4, 'type': 'PTV'},
                        'stop_lost': {'score': 5, 'type': 'STOP_LOST'},
                        'start_lost': {'score': 6, 'type': 'START_LOST'},
                        'inframe_insertion': {'score': 7, 'type': 'INFRAME'},
                        'inframe_deletion': {'score': 8, 'type': 'INFRAME'},
                        'missense_variant': {'score': 9, 'type': 'MISSENSE'},
                        'protein_altering_variant': {'score': 10, 'type': 'INFRAME'},
                        'splice_region_variant': {'score': 11, 'type': 'NONCODING'},
                        'incomplete_terminal_codon_variant': {'score': 12, 'type': 'INFRAME'},
                        'start_retained_variant': {'score': 13, 'type': 'SYN'},
                        'stop_retained_variant': {'score': 14, 'type': 'SYN'},
                        'synonymous_variant': {'score': 15, 'type': 'SYN'},
                        '5_prime_UTR_variant': {'score': 16, 'type': 'UTR'},
                        '3_prime_UTR_variant': {'score': 17, 'type': 'UTR'},
                        'intron_variant': {'score': 18, 'type': 'INTRONIC'},
                        'intergenic_variant': {'score': 19, 'type': 'INTERGENIC'},
                        'upstream_gene_variant': {'score': 20, 'type': 'INTERGENIC'},
                        'downstream_gene_variant': {'score': 21, 'type': 'INTERGENIC'},
                        'no_score': {'score': 22, 'type': 'ERROR'}}

    ret_csq = vep_consequences['no_score']
    for c in csqs_split:
        if c in vep_consequences:
            if vep_consequences[c]['score'] < ret_csq['score']:
                ret_csq = vep_consequences[c]

    return ret_csq


# Prepares a record for final printing by adding some additional pieces of information and
# finalising the names of some columns for easy printing.
def final_process_record(rec, severity):

    # Has to have a "#" to be compatible with VCF I/O
    rec['#CHROM'] = rec['CHROM']

    # Rename some columns for printing purposes
    rec['parsed_csq'] = severity['type']
    rec['parsed_csq_val'] = severity['score']

    # Records who do not have equal REF/ALT length are assigned as InDels
    if len(rec['REF']) != len(rec['ALT']):
        rec['is_indel'] = True
    else:
        rec['is_indel'] = False

    # By UKBB convention, only variants that are multiallelic have a ";" in the name
    if ';' in rec['ID']:
        rec['is_multiallelic'] = True
    else:
        rec['is_multiallelic'] = False

    # This corrects an issue with sites with 100% missingness that BCFtools doesn't handle correctly
    if rec['AF'] == '.':
        rec['AF'] = 0
        rec['FILTER'] = 'FAIL'
    else:
        rec['AF'] = float(rec['AF'])

    # Set sensible defaults for a variety of fields:
    # "gnomad_maf", "REVEL" "SIFT", "PolyPhen", "LoF"
    rec['gnomad_maf'] = rec['gnomad_maf'] if rec['gnomad_maf'] != '.' else '0' # Sites w/o gnomAD don't exist in gnomAD so a MAF of 0 seems appropriate
    rec['REVEL'] = rec['REVEL'] if rec['REVEL'] != '.' else 'NaN' # NaN is default VCF spec for missing floats
    rec['SIFT'] = rec['SIFT'] if rec['SIFT'] != '.' else 'NA' # NA is default VCF spec for missing strings
    rec['PolyPhen'] = rec['PolyPhen'] if rec['PolyPhen'] != '.' else 'NA' # NA is default VCF spec for missing strings
    rec['LoF'] = rec['LoF'] if rec['LoF'] != '.' else 'NA' # NA is default VCF spec for missing strings

    # Setting additional tags requested by GWAS-y people
    if float(rec['AF']) < 0.5:
        rec['minor_allele'] = rec['ALT']
        rec['major_allele'] = rec['REF']
        rec['MAF'] = rec['AF']
        rec['MAC'] = rec['AC']
    else:
        rec['minor_allele'] = rec['REF']
        rec['major_allele'] = rec['ALT']
        rec['MAF'] = '%s' % (1 - float(rec['AF']))
        rec['MAC'] = '%s' % (int(rec['AN']) - int(rec['AC']))

    return(rec)


# This function parses VEP output for most severe CSQ for each variant.
# See individual comments in this code to understand how that is done.
def parse_vep():

    # These are all possible fields from the vep table that we generated in run_vep()
    # And then read it in as a csv.DictReader()
    csv_reader_header = ("CHROM", "POS", "REF", "ALT", "ID", "FILTER", "AF", "prop_missing", "AN", "AC",
                         "mane_transcript", "ENST_ID", "ENSG_ID", "biotype","is_canonical", "symbol", "csq",
                         "gnomad_maf", "REVEL", "SIFT", "PolyPhen", "LoF")
    vep_reader = csv.DictReader(open('variants.vep_table.tsv', 'r', newline = '\n'), delimiter="\t", fieldnames = csv_reader_header, quoting = csv.QUOTE_NONE)

    # Next, open a file that will contain (in tabix format tsv) the info we want to add back to the vcf
    # And these are all possible output fields that we want
    annote_writer_header = ("#CHROM", "POS", "REF", "ALT", "mane_transcript", "ENST_ID", "ENSG_ID", "biotype", # OG Fields
                         "symbol", "csq", "gnomad_maf", "REVEL", "SIFT", "PolyPhen", "LoF", # OG Fields
                         "parsed_csq", "is_multiallelic", "is_indel", "minor_allele", "major_allele", "MAF", "MAC") # New Fields
    # Write the header for use with bcftools annotate
    write_annote_header()
    # And open the csv DictWriter to put annotations into
    annote_file = open('variants.vep_table.annote.tsv', 'w')
    annote_writer = csv.DictWriter(annote_file, delimiter = "\t", fieldnames = annote_writer_header, extrasaction='ignore')

    # Now we need to iterate through the .tsv file that we made
    # A single variant can be spread across multiple rows as it can be annotated for multiple transcripts in the same gene
    # The annotations ARE always one after another (all entries for one variant are sequential), so we don't have to
    # worry about the order of the file.
    # But we do have to collect multiple entries for one variant, and then decide which one is the most "important". So we:
    # 1. Iterate through some records until we find a record that is NOT the same ref/alt
    # 2. Decide if the severity of the current record is "worse" than the currently held_rec
    # 3. Write the record (function: final_process_record())
    # 4. Repeat steps 1 - 3 for the next record until we reach the end of the file
    held_rec_name = None
    held_rec = None
    held_severity_score = None

    # Iterate through records (step 1)
    for rec in vep_reader:

        # Set a unqiue record ID
        current_rec_name = '%s_%s_%s_%s' % (rec['CHROM'], rec['POS'], rec['REF'], rec['ALT'])

        if current_rec_name != held_rec_name: # If ID is not the same, write currently held record and reset (steps 3 - 4)
            if (held_rec_name != None): # Obviously, don't print if going through the first rec since there is no stored INFO yet
                # Write the record with the most severe consequence (step 3)
                held_rec = final_process_record(held_rec, held_severity_score)
                annote_writer.writerow(held_rec)

            # Reset to a new record (step 4)
            held_rec_name = current_rec_name
            held_rec = rec # Make sure at least one record makes it through
            # This function decides how "severe" a given CSQ annotation for a record is. See the function for more details
            held_severity_score = define_score(held_rec['csq'])
        else:
            # check to see if we should prioritise the new record based on the following ordered criteria (step 2):
            # Below are named in DECREASING selection importance
            # 1. protein_coding
            # 2. MANE Transcript
            # 3. VEP Canonical Transcript
            # 4. CSQ Severity
            # All "else" statements are when two records have identical annotations for the given category above
            if rec['biotype'] == 'protein_coding' and held_rec['biotype'] != 'protein_coding':
                held_rec = rec
                held_severity_score = define_score(held_rec['csq'])
            elif rec['biotype'] != 'protein_coding' and held_rec['biotype'] == 'protein_coding':
                held_rec = held_rec
            else:
                if rec['mane_transcript'] != '.' and held_rec['mane_transcript'] == '.':
                    held_rec = rec
                    held_severity_score = define_score(held_rec['csq'])
                elif rec['mane_transcript'] == '.' and held_rec['mane_transcript'] != '.':
                    held_rec = held_rec
                else:
                    if rec['is_canonical'] == 'YES' and held_rec['is_canonical'] == '.':
                        held_rec = rec
                        held_severity_score = define_score(held_rec['csq'])
                    elif rec['is_canonical'] == '.' and held_rec['is_canonical'] == 'YES':
                        held_rec = held_rec
                    else:
                        current_severity_score = define_score(rec['csq'])
                        if (current_severity_score['score'] < held_severity_score['score']):
                            held_rec = rec
                            held_severity_score = define_score(held_rec['csq'])

    # And print the last record since it cannot be compared to an old record above:
    held_rec = final_process_record(held_rec, held_severity_score)
    annote_writer.writerow(held_rec)

    # This flushes all data & closes the output since this is not done by default in csv.DictWriter
    annote_file.close()

    # bgzip/tabix the output(s) to save space on DNAnexus / allow postprocessing
    cmd = "bgzip /test/variants.vep_table.annote.tsv"
    run_cmd(cmd, True)
    cmd = "tabix -p vcf /test/variants.vep_table.annote.tsv.gz"
    run_cmd(cmd, True)


# This function simply runs bcftools annotate to add VEP information back to our QCd VCF
def annotate_vcf_with_vep(vcfprefix):

    # This is similar to how BCFtools annotate was run above to add gnomAD MAF but for A LOT more fields that we got via VEP
    cmd = "bcftools annotate --threads 4 -a /test/variants.vep_table.annote.tsv.gz -c " \
          "CHROM,POS,REF,ALT,MANE,ENST,ENSG,BIOTYPE,SYMBOL,CSQ,gnomAD_AF,REVEL,SIFT,POLYPHEN,LOFTEE,PARSED_CSQ,MULTI,INDEL,MINOR,MAJOR,MAF,MAC " \
          "-h /test/variants.header.txt -Oz -o /test/" + vcfprefix + ".norm.filtered.tagged.missingness_filtered.annotated.vcf.gz /test/variants.norm.filtered.tagged.missingness_filtered.vcf.gz"
    run_cmd(cmd, True)
    purge_file("variants.norm.filtered.tagged.missingness_filtered.vcf.gz")

    # Index the file for later processing
    cmd = "bcftools index --threads 4 -t /test/" + vcfprefix + ".norm.filtered.tagged.missingness_filtered.annotated.vcf.gz"
    run_cmd(cmd, True)


@dxpy.entry_point('main')
def main(vcf):

    # Download the VCF file chunk to the instance
    vcf = dxpy.DXFile(vcf)
    dxpy.download_dxfile(vcf.get_id(), "variants.vcf.gz")

    # Set a prefix name for all files so that we can output a standard-named file:
    vcfprefix = vcf.describe()['name'].split(".vcf.gz")[0]

    # Bring a prepared docker image into our environment so that we can run commands we need:
    # The Dockerfile to build this image is located at resources/Dockerfile
    cmd = "docker pull egardner413/mrcepid-filtering:latest"
    run_cmd(cmd)

    # Separate function to acquire necessary resource files
    ingest_resources()

    # Generate a normalised vcf.gz file for all downstream processing:
    # -m : splits all multiallelics into separate records
    # -f : provides a reference file so bcftools can left-normalise and check records against the reference genome
    cmd = "bcftools norm --threads 4 -Oz -o /test/variants.norm.vcf.gz -m - -f /test/reference.fasta /test/variants.vcf.gz"
    run_cmd(cmd, True)
    purge_file("variants.vcf.gz")

    # Now the actual work. We are doing two separate things with two separate VCF files:
    # 1. Filtering GENOTYPES to get a final list of sites and pass genotypes
    do_filtering(vcfprefix)

    # 2. Adding annotations from VEP
    run_vep()

    # 3. Generating merged/final files:
    # This function parses the information from the raw VEP run (via run_vep()) and adds it to our filtered vcf
    parse_vep()

    # 4. Annotate the final, filtered VCF with our VEP fields to make it easy to go back and generate files for association testing
    annotate_vcf_with_vep(vcfprefix)

    # Set output
    # The first two files are the actual product of this entire piece of code and the most important.
    # The second two are simple stats that we can use to make sure everything looks OK
    output_vcf = vcfprefix + ".norm.filtered.tagged.missingness_filtered.annotated.vcf.gz"
    output_vcf_idx = vcfprefix + ".norm.filtered.tagged.missingness_filtered.annotated.vcf.gz.tbi"
    site_stats_pre_filter = vcfprefix + ".filtered_site_stats.tsv.gz"
    site_stats_post_filter = vcfprefix + ".unfiltered_site_stats.tsv.gz"
    indv_stats_pre_filter = vcfprefix + ".filtered_sample_stats.tsv.gz"
    indv_stats_post_filter = vcfprefix + ".unfiltered_sample_stats.tsv.gz"

    # Getting files back into your project directory on DNAnexus is a two-step process:
    # 1. uploading the local file to the DNA nexus platform to assign it a file-ID (looks like file-ABCDEFGHIJKLMN1234567890)
    # 2. linking this file ID to your project and placing it within your project's directory structure
    # (the subdirectory can be controlled on the command-line by adding a flag to `dx run` like: --destination test/)
    output = {"output_vcf": dxpy.dxlink(dxpy.upload_local_file(output_vcf)),
              "output_vcf_idx": dxpy.dxlink(dxpy.upload_local_file(output_vcf_idx)),
              "site_stats_pre_filter": dxpy.dxlink(dxpy.upload_local_file(site_stats_pre_filter)),
              "site_stats_post_filter": dxpy.dxlink(dxpy.upload_local_file(site_stats_post_filter)),
              "indv_stats_pre_filter": dxpy.dxlink(dxpy.upload_local_file(indv_stats_pre_filter)),
              "indv_stats_post_filter": dxpy.dxlink(dxpy.upload_local_file(indv_stats_post_filter))}

    # This returns all the information about your exit files to the work managing your job via DNANexus:
    return output

dxpy.run()
