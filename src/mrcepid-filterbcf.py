#!/usr/bin/env python
# mrcepid-filterbcf 1.0.0
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import sys
import math
from time import sleep
from os.path import exists
from concurrent import futures

# We have to do this to get modules to run properly on DNANexus while still enabling easy editing in PyCharm
sys.path.append('/')
sys.path.append('/filterbcf/')

from filterbcf.ingest_data import *
from filterbcf.vcf_filter.vcf_filter import *
from filterbcf.vcf_annotate.vcf_annotate import *


class ProcessedReturn(TypedDict):
    chrom: str
    start: int
    end: int
    vcf_prefix: dxpy.DXFile
    output_bcf: dxpy.DXFile
    output_bcf_idx: dxpy.DXFile
    output_vep: dxpy.DXFile
    output_vep_idx: dxpy.DXFile
    output_per_sample: dxpy.DXFile


# This is a method that will execute all steps necessary to process one VCF file
# It is the primary unit that is executed by individual threads from the 'main()' method
def process_vcf(vcf: str) -> ProcessedReturn:

    # Create a DXFile instance of the given file:
    vcf = dxpy.DXFile(vcf)

    # Set a prefix name for all files so that we can output a standard-named file:
    vcfprefix = vcf.describe()['name'].split(".bcf")[0]

    # Download the VCF file chunk to the instance
    dxpy.download_dxfile(vcf.get_id(), vcfprefix + ".bcf")

    # 1. Do normalisation and filtering
    print("Filtering bcf: " + vcf.describe()['name'])
    VCFFilter(vcfprefix)

    # We need to pause here in each thread to make sure that CADD and VEP files have downloaded in separate threads...
    # We know that when the original .tar.gz files are gone, that it is safe as deleting those files is the final step
    # of the download process.
    vep_tar = 'vep_caches/vep_cache.tar.gz'
    cadd_tar = 'cadd_files/annotationsGRCh38_v1.6.tar.gz'
    precomputed_index = 'cadd_precomputed/gnomad.genomes.r3.0.indel.tsv.gz.tbi'
    while (exists(vep_tar) or exists(cadd_tar)) and exists(precomputed_index) is False:
        sleep(5)

    # 2. Do annotation
    print("Annotating bcf: " + vcf.describe()['name'])
    vcf_annotater = VCFAnnotate(vcfprefix)

    print("Finished bcf: " + vcf.describe()['name'])

    return {'chrom': vcf_annotater.chunk_chrom,
            'start': vcf_annotater.chunk_start,
            'end': vcf_annotater.chunk_end,
            'vcf_prefix': vcfprefix,
            'output_bcf': vcf_annotater.finalbcf,
            'output_bcf_idx': vcf_annotater.finalbcf_index,
            'output_vep': vcf_annotater.finalvep,
            'output_vep_idx': vcf_annotater.finalvep_index,
            'output_per_sample': vcf_annotater.output_per_sample}


@dxpy.entry_point('main')
def main(input_vcfs, coordinates_name, human_reference, human_reference_index, vep_cache, loftee_libraries,
         gnomad_maf_db, revel_db,
         cadd_annotations, precomputed_cadd_snvs, precomputed_cadd_indels):

    # Now build a thread worker that contains as many threads, divided by 2 that have been requested since each bcftools
    # 1 thread for monitoring threads
    # 2 threads for downloading (1 each for CADD and VEP)
    # 2 threads for each BCF
    available_workers = math.floor((get_thread_number() - 1) / 2)
    executor = ThreadPoolExecutor(max_workers=available_workers)

    # Separate function to acquire necessary resource files
    ingested_data = IngestData(executor, input_vcfs, human_reference, human_reference_index, vep_cache, loftee_libraries,
                               gnomad_maf_db, revel_db,
                               cadd_annotations, precomputed_cadd_snvs, precomputed_cadd_indels)

    # And launch the requested threads
    future_pool = []
    for input_vcf in ingested_data.input_vcfs:
        future_pool.append(executor.submit(process_vcf, vcf=input_vcf))
    print("All threads submitted...")

    # And add the resulting futures to relevant output arrays / file
    output_bcfs = []
    output_bcf_idxs = []
    output_veps = []
    output_vep_idxs = []
    output_per_samples = []

    # This file contains information about the 'chunks' that have been processed. It DOES NOT have a header to
    # make it easier to concatenate coordinate files from multiple runs. The columns are as follows:
    # [0] #chrom
    # [1] start
    # [2] end
    # [3] chunk_prefix
    # [4] bcf_dxpy
    # [5] vep_dxpy
    coordinate_writer = open(coordinates_name, 'w')

    # And gather the resulting futures
    for future in futures.as_completed(future_pool):
        try:
            result = future.result()
            output_bcfs.append(result['output_bcf'])
            output_bcf_idxs.append(result['output_bcf_idx'])
            output_veps.append(result['output_vep'])
            output_vep_idxs.append(result['output_vep_idx'])
            output_per_samples.append(result['output_per_sample'])
            coordinate_writer.write('%s\t%i\t%i\t%s\t%s\t%s\n' % (result['chrom'],
                                                                  result['start'],
                                                                  result['end'],
                                                                  result['vcf_prefix'],
                                                                  result['output_bcf'].describe()['id'],
                                                                  result['output_vep'].describe()['id']))
        except Exception as err:
            print("A thread failed...")
            traceback.format_exc()
            print(Exception, err)

    coordinate_writer.close()

    print("All threads completed...")

    # Getting files back into your project directory on DNAnexus is a two-step process:
    # 1. uploading the local file to the DNA nexus platform to assign it a file-ID (looks like file-ABCDEFGHIJKLMN1234567890)
    # 2. linking this file ID to your project and placing it within your project's directory structure
    # (the subdirectory can be controlled on the command-line by adding a flag to `dx run` like: --destination test/)
    output = {"output_bcfs": [dxpy.dxlink(item) for item in output_bcfs],
              "output_bcf_idxs": [dxpy.dxlink(item) for item in output_bcf_idxs],
              "output_veps": [dxpy.dxlink(item) for item in output_veps],
              "output_vep_idxs": [dxpy.dxlink(item) for item in output_vep_idxs],
              "output_per_samples": [dxpy.dxlink(item) for item in output_per_samples],
              "coordinates_file": dxpy.dxlink(dxpy.upload_local_file(coordinates_name))}

    # This returns all the information about your exit files to the work managing your job via DNANexus:
    return output


dxpy.run()
